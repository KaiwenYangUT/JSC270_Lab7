{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaiwenYangUT/JSC270_Lab7/blob/main/JSC270_2023_Lab7_NLP_Intro_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGb7zK-ynoaY"
      },
      "source": [
        "# JSC270 Lab 7: Intro to Natural Language Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si_aWMP8n5cr"
      },
      "source": [
        "## Some announcements before we start:\n",
        "- The code (jupyter notebook) from Alastair Johnson's lecture on Monday has been posted to the class website. The notebook generates his slides.\n",
        "\n",
        "- Assignment 3 is due on Friday at 10:59am.\n",
        "\n",
        "- The first part of Assignment 4 is to find a group member. Please do so and sign up on this sheet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzjkN4q4pN9E"
      },
      "source": [
        "## 1. Working with Strings in Python\n",
        "\n",
        "- In python (and most other programming languages), we represent text using __strings__, which are just sequences of characters stored as arrays\n",
        "- You can assign a string object in python using single or double quotes\n",
        "- Triple quotes can be used for a multiline strings\n",
        "- Strings can be concatenated (added together) with the '+' symbol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uye8hHFFus4j",
        "outputId": "dff477ab-40b8-49b8-9cab-6252bf7f915c"
      },
      "source": [
        "string1 = 'This is how we store text'\n",
        "string2 = \"We could store text like this too, with double quotes\"\n",
        "string3 = '''I'm a multiline string. This is very useful for representing \n",
        "larger volumes of text, like paragraphs of a book.'''\n",
        "\n",
        "\n",
        "print(string1 + '\\n', string2 + '\\n', string3)\n",
        "print(string1 + '. ' + string2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is how we store text\n",
            " We could store text like this too, with double quotes\n",
            " I'm a multiline string. This is very useful for representing \n",
            "larger volumes of text, like paragraphs of a book.\n",
            "This is how we store text. We could store text like this too, with double quotes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRvu8pQ57X4O"
      },
      "source": [
        "- Because python strings are represented as arrays, we can also iterate and slice them, just like with a regular array (i.e. a list)\n",
        "- Since strings are represented with quotation marks, if we ever want quotes within the string itself, we need to use the backslash (called the escape character)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUqJCSQk7yCo",
        "outputId": "6088b562-fdea-40e1-8875-3be088ff3675"
      },
      "source": [
        "# print the first 5 characters of string 3\n",
        "print(string3[:5])\n",
        "\n",
        "# Iterate through the first 4 characters of string 1\n",
        "for i in range(5):\n",
        "  print(string1[i])\n",
        "\n",
        "# Create a string with quotation marks\n",
        "quote_string = 'This string \\'actually\\' has quotation marks in it'\n",
        "print(quote_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm a\n",
            "T\n",
            "h\n",
            "i\n",
            "s\n",
            " \n",
            "This string 'actually' has quotation marks in it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkfnLzc58Sra"
      },
      "source": [
        "- Note that in the printout, the 5th element of `string1` was printed. It just happened to be whitespace, which is treated as its character. This is an important detail - hidden whitespace can make it difficult for you to clean text data.\n",
        "\n",
        "- Python also has a set of methods designed for you to modify existing strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsnjHhTy88AD",
        "outputId": "faeee981-b92c-49ba-8395-7b18ec48b11f"
      },
      "source": [
        "a = 'THIS STRING IS ALL CAPITAL LETTERS'\n",
        "b = 'this string has all lowercase letters'\n",
        "c = '     This string has extra whitespace   '\n",
        "d = 'This string has exactly zne misspelling'\n",
        "\n",
        "# Convert a to lowercase\n",
        "print('Convert a to lowercase:',a.lower())\n",
        "\n",
        "# Convert b into capitals\n",
        "print('Convert b to capitals:', b.upper())\n",
        "\n",
        "# Remove excess whitespace from beginning and end of string\n",
        "print('Take the whitespace off the beg and end of c:', c.strip())\n",
        "\n",
        "# Replace characters as needed\n",
        "print('Fix the typo in d:',d.replace('z','o'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a to lowercase: this string is all capital letters\n",
            "Convert b to capitals: THIS STRING HAS ALL LOWERCASE LETTERS\n",
            "Take the whitespace off the beg and end of c: This string has extra whitespace\n",
            "Fix the typo in d: This string has exactly one misspelling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s7dYzyy_jQD"
      },
      "source": [
        "- Some of these methods can accept as inputs strings longer than one character\n",
        "- Just be careful: Methods act over the entire string, so you may modify parts of text you didn't expect\n",
        "- You can also chain these methods together to create a small pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4ma4uLE5_80a",
        "outputId": "11107156-6209-44d5-8c1d-a07527c7fee7"
      },
      "source": [
        "d.replace('z','o').replace('one', 'zero').replace('misspelling','misspellings')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This string has exactly zero misspellings'"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4hchYJ7Any0"
      },
      "source": [
        "- We can use the `split` method to split a string of running text into a list of words. By default, it separates by whitespace. But you can give a different delimiter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxAJYH8fD3Q1",
        "outputId": "36735cdc-beee-4332-cce2-d13dc80f98a8"
      },
      "source": [
        "s = 'This is a list of words we might want to separate.'\n",
        "\n",
        "# Separate string s into a list of words\n",
        "s.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'list',\n",
              " 'of',\n",
              " 'words',\n",
              " 'we',\n",
              " 'might',\n",
              " 'want',\n",
              " 'to',\n",
              " 'separate.']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXnvLsGVEHqm"
      },
      "source": [
        "Notice that we did not remove the punctuation. We'll look at how to do that shortly.\n",
        "- There are many other useful methods not covered here (Google is your friend here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmS83pWxEUdS"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "__Q: Based on what you've just seen, how might we remove the punctuation?__ There's more than one right answer here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfSDpYF9Elj-"
      },
      "source": [
        "## Answer\n",
        "s = s.replace('.','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PSI6boOo8nd"
      },
      "source": [
        "## 2. Pattern Identification with Regular Expressions\n",
        "\n",
        "- What if we didn't create the string, but we wanted to find it based on a specific pattern it might contain?\n",
        "\n",
        "- __Regular Expressions__, or regex for short, are sequences of characters designed to specify search patterns. They can be used to find strings containing specific patterns, or to replace specific parts of a string. These patterns are defined at the character level.\n",
        "\n",
        "- Regex uses certain metacharacters to span larger string patterns with short syntax. For example:\n",
        "    - `[a-q]` is used to denote any (lowercase) letter between a and q\n",
        "    - `\\w` is used to denote any alphanumeric character (any number or letter)\n",
        "\n",
        "- Virtually every programming language has some sort of regex implementation. We'll use python's `re` package\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNxOIyLrSVYd"
      },
      "source": [
        "import re\n",
        "\n",
        "# Let's define some strings, representing what we might see text\n",
        "\n",
        "a = 'half a league, half a league, half a league onward!'\n",
        "b = 'John Smith, Toronto, Canada, L2G0C8'\n",
        "c = 'assignment1.csv, assignment2.txt, assignment3.pdf'\n",
        "d = 'The review committee for 2018 approved a travel budget of $2800'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAuOQdj-W3rG"
      },
      "source": [
        "We can use regex to identify (and possibly remove) certain parts of these strings. The following are commonly used patterns:\n",
        "- `[A-Zz-z]` denotes all letters (capital and lowercase)\n",
        "- `[0-5][5-9]` denotes any 2 digit number between 05 and 59\n",
        "- `[abcd]` denotes a or b or c or d (but only one of them) \n",
        "- `[^def]` denotes any character that is not d or e or f\n",
        "- `.` The period is a wildcard - denotes any character\n",
        "- `\\w` denotes any alphanumeric character\n",
        "- `\\s` denotes whitespace\n",
        "- `\\S` denotes any non-whitespace character\n",
        "\n",
        "Using the strings defined above, let's try and find/extract information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lixWbFI_ZYez",
        "outputId": "1a034324-01ab-4b73-f857-3d9816eede88"
      },
      "source": [
        "#### How often does the phrase 'Half a league' occur in a? ####\n",
        "result = re.findall('[Hh]alf a league', a)\n",
        "print(result)\n",
        "\n",
        "# This function returns a list of all the matches for my search (Note the regex used)\n",
        "print('The phrase \\'half a league\\' occurs {} times in a'.format(len(result)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['half a league', 'half a league', 'half a league']\n",
            "The phrase 'half a league' occurs 3 times in a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9Tf1XZgbMpM",
        "outputId": "4714550b-3dc6-401a-980e-1d21ca8932a4"
      },
      "source": [
        "#### What types of files are contained in c? #####\n",
        "result = re.findall('\\.([a-z]{3})', c)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['csv', 'txt', 'pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAZRNUaecR03"
      },
      "source": [
        "For that last example, notice:\n",
        "- Since the `.` is a wildcard character by default, if I actually want to look for a period, I need to use the backslash (escape character) to denote it\n",
        "- The `{}` are used as shorthand to repeat a character within the search pattern (in this case I was look for three digits in a row).\n",
        "- The `()` form what's called a __capture group__. This is used to separate the pattern I searched for from the information I actually need. In this case, the period is necessary to find the file formats, but I don't actually want it returned with the other three digits, so I exclude it from the capture group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86qa13A9dEuL",
        "outputId": "76fddb64-ef4c-436e-ddb7-ac37d3b9909f"
      },
      "source": [
        "#### Is the budget in d for 2018 greater than $1000?\n",
        "budget = re.findall('\\$([0-9]+)',d)\n",
        "\n",
        "print(budget)\n",
        "print( (float(budget[0]) > 1000) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2800']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pscBU5sFido7"
      },
      "source": [
        "Note the `+`, which is called the Kleene Plus. It is used to indicate '1 or more' of some character. The `*` is called the Kleene star, and is used to specify 'zero or more'. With both of these symbols, there is no upper limit (ie `[0-9]+` could potentially capture a string of 10000 digits if that exists)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4UkYco6jJla",
        "outputId": "64806cd5-fb76-420e-9915-0fd6694d2332"
      },
      "source": [
        "#### Replace the year and budget in d with 2019 and $30000 ####\n",
        "print('Original string:\\n', d)\n",
        "new_budget = re.sub('\\$[0-9]+','$30000', d)\n",
        "print('Changed budget:\\n',new_budget)\n",
        "\n",
        "new_year = re.sub('\\s\\d+\\s', ' 2019 ', new_budget)\n",
        "print('Both changes:\\n',new_year)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string:\n",
            " The review committee for 2018 approved a travel budget of $2800\n",
            "Changed budget:\n",
            " The review committee for 2018 approved a travel budget of $30000\n",
            "Both changes:\n",
            " The review committee for 2019 approved a travel budget of $30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkK4N0KBkjga"
      },
      "source": [
        "Remember that `\\s` denotes whitespace. `\\d` is short notation for any digit `[0-9]`. The regex for new year can be read as \"[Whitespace][One or more digits][Whitespace]\".\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "__Q: What would happen in the above example if I didn't include the whitespace indicators in my regular expression?__ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfmGBgxyk91q",
        "outputId": "8f6128f6-9403-4dab-c08b-fb0676b040b6"
      },
      "source": [
        "# Answer\n",
        "new_year = re.sub('[0-9]{4}', '2019', new_budget)\n",
        "print('Both changes:\\n',new_year)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Both changes:\n",
            " The review committee for 2019 approved a travel budget of $20190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnkwocnal4vM"
      },
      "source": [
        "<br> \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiTvozN8lhOu",
        "outputId": "bf8ea5a5-7f03-41ab-d518-7f84ae778c11"
      },
      "source": [
        "#### Extract the name and Postal code from b ####\n",
        "print(b)\n",
        "\n",
        "# There may be ways to do this\n",
        "post_code = re.search('[A-Z]\\d[A-Z]\\s?\\d[A-Z]\\d', b)\n",
        "print('Postal Code:', post_code.group())\n",
        "\n",
        "# Names will almost always start with capital letters\n",
        "# Also, there will be two of them consecutively\n",
        "\n",
        "name = re.search('[A-Z][a-z]+\\s+[A-Z][a-z]+', b)\n",
        "print('Name:', name.group())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John Smith, Toronto, Canada, L2G0C8\n",
            "Postal Code: L2G0C8\n",
            "Name: John Smith\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gii8vAkOuQpX"
      },
      "source": [
        "Notice for the regex in the postal code extraction, I simply followed the pattern of a postal code (letter, number, letter, ...). The `?` symbol makes the preceding character optional. Since some postal codes contain spaces, this search pattern is now robust to that possibility.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "__Q: What would be the regular expression for extracting Twitter handles?(e.g. @xyz)__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuZ-L7zLu4t1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46e85655-79a3-4f68-e4a0-7487bd656de7"
      },
      "source": [
        "# Answer\n",
        "'@\\w+'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'@\\\\w+'"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2B2PyTIu8Tu"
      },
      "source": [
        "- The ones we've seen today are just a small portion of the whole language. I encourage you to read further into them, since you will likely use them in later courses.\n",
        "- __Regexone__ hosts a great sequence of tutorials on regular expressions, which you can visit [here](https://regexone.com/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-La_D2XpYV8"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3. Today's Dataset \n",
        "\n",
        "- Today we'll be doing binary classification (spam detection) with email messages. Our dataset is a subsample of a larger set that comes from the Machine Learning Repository at UC Irvine. This particular data (as is often the case) was used for a Kaggle competition, which you can read about [here](https://www.kaggle.com/uciml/sms-spam-collection-dataset/tasks). \n",
        "\n",
        "- The dataset contains 5572 observations, with the message itself, and the label (spam/ham). The file `spam.csv` is listed alongside the link to this notebook. \n",
        "\n",
        "<br>\n",
        "\n",
        "__Go ahead and download the dataset from [here](https://drive.google.com/file/d/10vSo6HaJDVqCgti07q87KkpCMYzdJf6v/view?usp=sharing). You can load it into this notebook using the code below__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VqgP9BB3oOV",
        "outputId": "b5a0afe3-9a0b-4b98-a1a8-28fc58c19d34"
      },
      "source": [
        "# Read in data from github \n",
        "!wget \"https://raw.githubusercontent.com/jlgrons/JSC270_Lab7/master/spam.csv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-06 21:52:10--  https://raw.githubusercontent.com/jlgrons/JSC270_Lab7/master/spam.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [text/plain]\n",
            "Saving to: ‘spam.csv.3’\n",
            "\n",
            "\rspam.csv.3            0%[                    ]       0  --.-KB/s               \rspam.csv.3          100%[===================>] 491.86K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-03-06 21:52:10 (16.9 MB/s) - ‘spam.csv.3’ saved [503663/503663]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "df = pd.read_csv(r\"/content/spam.csv\", encoding= 'latin1')"
      ],
      "metadata": {
        "id": "un6kQXLrLIbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGonL-ko3_4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3120aff0-95e5-4306-dc52-a64e5609347c"
      },
      "source": [
        "# You'll know you were successful if this code works\n",
        "import numpy as np\n",
        "\n",
        "print(df.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2 Unnamed: 2  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKXgvWLS5xms"
      },
      "source": [
        "- Note the `encoding` argument passed to the `read_csv` method. CSV files can often be encoded in a number of different ways. The standard is `encoding = 'utf-8'`, however there are other systems, like `'latin1'`.\n",
        "\n",
        "You can run the following code to get an idea of what we're dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W23qliQu6vK7",
        "outputId": "ed1c19c1-b294-437f-e459-6f15d9457be2"
      },
      "source": [
        "# This function will print a random email message\n",
        "def print_random_message():\n",
        "  i = np.random.randint(len(df))\n",
        "  message = df.iloc[i,1]\n",
        "  label = df.iloc[i,0]\n",
        "  print('Here is one of the messages:\\n', message)\n",
        "  print('It is labelled as:\\n',label)\n",
        "\n",
        "# call the function\n",
        "print_random_message()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is one of the messages:\n",
            " Que pases un buen tiempo or something like that\n",
            "It is labelled as:\n",
            " ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnp8xf7N7IE7"
      },
      "source": [
        "Before we do anything with the text itself, the df needs a bit of tidying. The default parser gave us some extra columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5YWh6Vt7T2c",
        "outputId": "defd0586-a8ec-4288-c9e6-795fdcd8e25d"
      },
      "source": [
        "# How many columns are there now?\n",
        "print('The columns read in are:',list(df))\n",
        "\n",
        "# The datset should only contain {label, message}\n",
        "data = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)\n",
        "# rename other columns while we're at it\n",
        "data.columns = ['label', 'message']\n",
        "print('Now we keep just what we need:\\n', data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The columns read in are: ['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
            "Now we keep just what we need:\n",
            "   label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIjxagJy9S8_"
      },
      "source": [
        "The last thing I'm going to do before we look at the text is change the labels to numeric (1 = spam). Remember, in binary classification we usually want to make the minority outcome the positive class, since it is usually what we're interested in. While I'm at it, let's examine the balance between the classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoYaMK4n9nuD",
        "outputId": "afedcd9f-c11c-4978-ab6b-af79cc55e090"
      },
      "source": [
        "# Change spam = 1, ham = 0\n",
        "data.label = (data.label == 'spam').astype(float)\n",
        "\n",
        "print(data.head(5),'\\n\\n')\n",
        "\n",
        "# Compute proportion of spam observations\n",
        "spam_proportion = round(data.label.sum()/len(data), 2)\n",
        "print('Spam labels make up approximately {} percent of the dataset'.format(spam_proportion))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                            message\n",
            "0    0.0  Go until jurong point, crazy.. Available only ...\n",
            "1    0.0                      Ok lar... Joking wif u oni...\n",
            "2    1.0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3    0.0  U dun say so early hor... U c already then say...\n",
            "4    0.0  Nah I don't think he goes to usf, he lives aro... \n",
            "\n",
            "\n",
            "Spam labels make up approximately 0.13 percent of the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCLU5PJw_M4h"
      },
      "source": [
        "So we have about an 87/13 split, which is very imbalanced. Keep this in mind later, when we fit a classifier and evaluate its performance. Let's see an example of each type of message:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrJY8WjYB727",
        "outputId": "da3cc882-22d6-4bc7-9281-54e45e41fe65"
      },
      "source": [
        "np.random.seed(16)\n",
        "print_random_message()\n",
        "print_random_message()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is one of the messages:\n",
            " Shop till u Drop, IS IT YOU, either 10K, 5K, å£500 Cash or å£100 Travel voucher, Call now, 09064011000. NTT PO Box CR01327BT fixedline Cost 150ppm mobile vary\n",
            "It is labelled as:\n",
            " spam\n",
            "Here is one of the messages:\n",
            " Ok anyway no need to change with what you said\n",
            "It is labelled as:\n",
            " ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NEkVTryB_TO"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "__Q: Looking at the two messages above, and using your own intuition, what characteristics do you think SPAM emails have that HAM emails do not?__ \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer"
      ],
      "metadata": {
        "id": "pY9W5StMG-Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlBmzSUOn_MX"
      },
      "source": [
        "## 4. Tokenization and Cleaning\n",
        "\n",
        "<br>\n",
        "\n",
        "- In terms of predictive power, using an entire sentence (or series of sentences) is not very effective. Most of the words won't matter for predicting spam, but certain groups of characters can be very important. First, some terminology: \n",
        "\n",
        "- In NLP, we call a single observation of text a __document__. In this case, our documents are emails, but they could be tweets, patents, cheques, web pages, etc...\n",
        "\n",
        "- A collection of documents (ie an entire text dataset) is called a __corpus__. There are many publically available corpora online, if you ever want to build your own models\n",
        "\n",
        "- We are interesting in splitting the documents into useful, informative units called __tokens__. Tokens are often just words, but they don't have to be. For instance, the token `'t`, when present to the right of the word `can`, will completely change the meaning of a sentence. The process of converting raw text into tokens is called __tokenization__.\n",
        "\n",
        "<br>\n",
        "\n",
        "There are a handful of great python libraries designed specifically for processing text. The one we'll use today is called `nltk`, which stands for Natural Language Tool Kit. \n",
        "- The first thing we'll want to do is tokenize the documents in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIAlPKNFu8KX",
        "outputId": "9afebff2-1255-4ef9-9764-25a811726eb8"
      },
      "source": [
        "import nltk\n",
        "# Download the tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Create a new column in our DF that contains token lists instead of raw text\n",
        "data['tokens'] = data['message'].apply(nltk.word_tokenize)\n",
        "\n",
        "print(data['tokens'].head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "0    [Go, until, jurong, point, ,, crazy.., Availab...\n",
            "1             [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
            "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
            "3    [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
            "4    [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
            "Name: tokens, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icbb2mkIJn2Y"
      },
      "source": [
        "Notice the 5th row of our printout: nltk's word tokenizer has automatically recognized that `n't` should be its own token. This would by very difficult to do by hand.\n",
        "\n",
        "- Another step we might want to take is to convert all our tokens into lowercase. This prevents a word at the beginning of a sentence from being treated as a separate token from other occurrences later in sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ThIIP9haNNN",
        "outputId": "fe8cce64-3c07-4832-9460-b8d0a20352e4"
      },
      "source": [
        "##### Convert tokens into lowercase ####\n",
        "lowercase_tokens = []\n",
        "# Create a list of lists with what we want\n",
        "for row in data['tokens']:\n",
        "  lowercase_tokens.append([t.lower() for t in row])\n",
        "# add the new info to our df\n",
        "data['lowercase_tokens'] = lowercase_tokens\n",
        "\n",
        "print(data['lowercase_tokens'].head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [go, until, jurong, point, ,, crazy.., availab...\n",
            "1             [ok, lar, ..., joking, wif, u, oni, ...]\n",
            "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
            "3    [u, dun, say, so, early, hor, ..., u, c, alrea...\n",
            "4    [nah, i, do, n't, think, he, goes, to, usf, ,,...\n",
            "Name: lowercase_tokens, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8gbSCNWfFF4"
      },
      "source": [
        "Now all tokens are lowercase. We could have also done these last two steps in opposite order, converting raw strings to lowercase and then tokenizing.\n",
        "\n",
        "<br>\n",
        "\n",
        "- Another thing we might want to do is remove punctuation. Again, since some tokens may have punctuation marks attached, this might prevent otherwise identical words from being treated as identical. We can use the regex functions to help here. Instead of listing all forms of punctuation (some of which, like `.` or `?`, would require the escape character), we can be a bit more clever, and remove all punctuation and special characters at once.\n",
        "- Remember that `\\w` is used to denote any alphanumeric character (it also allows underscores) and `\\s` denotes whitespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_oLg_YSf7cN",
        "outputId": "52103848-6dcb-45ac-dd95-2d6901d8a0f8"
      },
      "source": [
        "##### Let's remove punctuation #####\n",
        "\n",
        "# Note we've been keeping different columns for different steps (not necessary)\n",
        "list(data)\n",
        "\n",
        "# Same process as before\n",
        "tokens_no_punct = []\n",
        "# Create a list of lists with what we want\n",
        "for row in data['lowercase_tokens']:\n",
        "  tokens_no_punct.append([re.sub('[^\\w\\s]','', t) for t in row])\n",
        "# add the new info to our df\n",
        "data['tokens_no_punct'] = tokens_no_punct\n",
        "\n",
        "print(data['tokens_no_punct'].head(5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [go, until, jurong, point, , crazy, available,...\n",
            "1                   [ok, lar, , joking, wif, u, oni, ]\n",
            "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
            "3    [u, dun, say, so, early, hor, , u, c, already,...\n",
            "4    [nah, i, do, nt, think, he, goes, to, usf, , h...\n",
            "Name: tokens_no_punct, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OQNE9gh01Y"
      },
      "source": [
        "The carat symbol `^` is used for negation, so the regex in the above process is read as: 'Remove all characters that are not alphanumeric or whitespace'. We do this for every token in the list, and for each token list in our DF.\n",
        "\n",
        "- Be careful when removing all special characters. Sometimes certain symbols can have important meaning, and you dont' want them removed. In that case, your search pattern will have to be a bit more refined.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "__Q: Can you think of an example where we might want to keep a certain special character?__ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XwxsULdi4ph"
      },
      "source": [
        "# Answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOA1iGL9kXSF"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "Another common step is to remove words that we know will be in almost every document. These are called __stopwords__, and because they are used so often, they won't have any predictive power in most models (there are some rare exceptions, but they do not apply here). To avoid adding unnecessary noise, we can remove such words. `nltk` provides a list of almost 200 such words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO5ZYP97lIwI",
        "outputId": "52789455-4b04-4ad4-95be-22f0fa684050"
      },
      "source": [
        "##### Time to remove Stopwords #####\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# print the top 75 most popular english words\n",
        "sw = stopwords.words('english')[:75]\n",
        "\n",
        "# I converted to np array for better printing\n",
        "print(np.array(sw).reshape((15,5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[['i' 'me' 'my' 'myself' 'we']\n",
            " ['our' 'ours' 'ourselves' 'you' \"you're\"]\n",
            " [\"you've\" \"you'll\" \"you'd\" 'your' 'yours']\n",
            " ['yourself' 'yourselves' 'he' 'him' 'his']\n",
            " ['himself' 'she' \"she's\" 'her' 'hers']\n",
            " ['herself' 'it' \"it's\" 'its' 'itself']\n",
            " ['they' 'them' 'their' 'theirs' 'themselves']\n",
            " ['what' 'which' 'who' 'whom' 'this']\n",
            " ['that' \"that'll\" 'these' 'those' 'am']\n",
            " ['is' 'are' 'was' 'were' 'be']\n",
            " ['been' 'being' 'have' 'has' 'had']\n",
            " ['having' 'do' 'does' 'did' 'doing']\n",
            " ['a' 'an' 'the' 'and' 'but']\n",
            " ['if' 'or' 'because' 'as' 'until']\n",
            " ['while' 'of' 'at' 'by' 'for']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JjINa6XmhWH",
        "outputId": "bd71ccd6-9c03-4d2d-de05-ba7e23465fd6"
      },
      "source": [
        "# Now let's remove them\n",
        "tokens_no_sw = []\n",
        "for row in data['tokens_no_punct']:\n",
        "  tokens_no_sw.append([w for w in row if w not in sw])\n",
        "# Add column to df\n",
        "data['tokens_no_sw'] = tokens_no_sw\n",
        "\n",
        "# Print some examples\n",
        "print(data['tokens_no_sw'].tail(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5567    [2nd, time, tried, 2, contact, u, u, won, å750...\n",
            "5568      [will, ì_, b, going, to, esplanade, fr, home, ]\n",
            "5569    [pity, , , in, mood, , so, , any, other, sugge...\n",
            "5570    [guy, some, bitching, acted, like, d, interest...\n",
            "5571                             [rofl, , true, to, name]\n",
            "Name: tokens_no_sw, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONC95oC2nQGy"
      },
      "source": [
        "This looks good (but not great). Clearly there are other steps we could take (for instance, treating 'u' and 'you' as equal. But for the sake of time, we'll move on for now. In practice, it is very difficult to get a perfectly clean text dataset (things like typos/spelling errors, shorthand, slang, and redundancy (!?!?!!!) would still need to be dealt with).\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "__Q: With this analysis, we've assumed that the basic unit of interest is an individual word (or something representing one). Can you think of a case where we might want to use tokens made of more than 1 word?__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k0i71nPoVQR"
      },
      "source": [
        "<br>\n",
        "\n",
        "__Answer__: Multi-word tokens are called __N-grams__ (N is the number of words). So far we've been using __unigrams__, but in practice people often use __bigrams__, __trigrams__, or more. Examples:\n",
        "\n",
        "- Organizations (Employment and Social Development Canada)\n",
        "- Black Lives Matter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZyRfGlonYK"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 5. Stemming and Lemmatization\n",
        "\n",
        "In language processing, __stemming__ is the process of removing the suffixes of a word, thus reducing the word to its stem. This allows different tenses of the same word to be treated as the same token. Most stemmers are written as huge lists of handmapped words/tokens. Perhaps the most common of these is the __Porter Stemmer__. Again, `nltk` has us covered here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcL-d-kItvDF",
        "outputId": "197f0b6c-3551-4282-d197-9d6eaad31148"
      },
      "source": [
        "#### Stemming tokens ####\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "my_tokens = ['run','running', 'hike', 'hiking', 'apple', 'apples', 'knife', 'knives']\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "token_stems = [stemmer.stem(w) for w in my_tokens]\n",
        "\n",
        "print('Original tokens:\\n',my_tokens)\n",
        "print('Stemmed tokens',token_stems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens:\n",
            " ['run', 'running', 'hike', 'hiking', 'apple', 'apples', 'knife', 'knives']\n",
            "Stemmed tokens ['run', 'run', 'hike', 'hike', 'appl', 'appl', 'knife', 'knive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziaTyQ1Hu4Ie"
      },
      "source": [
        "We can see that stemming helps in some cases, but it is not perfect. The Porter stemmer is not our only option:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFMQuGOlu_Xx",
        "outputId": "1e120814-3483-4628-9c39-1dbb58b5123b"
      },
      "source": [
        "#### Let's try a SnowBall Stemmer ####\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "token_stems = [stemmer.stem(w) for w in my_tokens]\n",
        "\n",
        "print('Original tokens:\\n',my_tokens)\n",
        "print('Stemmed tokens',token_stems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens:\n",
            " ['run', 'running', 'hike', 'hiking', 'apple', 'apples', 'knife', 'knives']\n",
            "Stemmed tokens ['run', 'run', 'hike', 'hike', 'appl', 'appl', 'knife', 'knive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42BQtyLvYMX"
      },
      "source": [
        "We can see that most stemmers run into the same kind of problems with english irregularities. Nevertheless, stemming is effective for most regular word forms, so let's use it on our data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hObYI-i3vz19",
        "outputId": "8b9be65c-b774-474d-bc1c-5e6af195d353"
      },
      "source": [
        "### Stemming our dataset (Snowball Stemmer) ###\n",
        "stemmed_tokens = []\n",
        "for row in data['tokens_no_sw']:\n",
        "  stemmed_tokens.append([stemmer.stem(t) for t in row])\n",
        "\n",
        "data['stemmed_tokens'] = stemmed_tokens\n",
        "\n",
        "# Print results\n",
        "print('Before stemming:\\n', data['tokens_no_sw'].head(3))\n",
        "print('After stemming:\\n', data['stemmed_tokens'].head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before stemming:\n",
            " 0    [go, jurong, point, , crazy, available, only, ...\n",
            "1                   [ok, lar, , joking, wif, u, oni, ]\n",
            "2    [free, entry, in, 2, wkly, comp, to, win, fa, ...\n",
            "Name: tokens_no_sw, dtype: object\n",
            "After stemming:\n",
            " 0    [go, jurong, point, , crazi, avail, onli, in, ...\n",
            "1                     [ok, lar, , joke, wif, u, oni, ]\n",
            "2    [free, entri, in, 2, wkli, comp, to, win, fa, ...\n",
            "Name: stemmed_tokens, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AMpW_DjxAVH"
      },
      "source": [
        "The result is what we might expect. Some good, some bad. Is there a more thorough approach?\n",
        "\n",
        "<br>\n",
        "\n",
        "__Lemmatization__ converts a word/token to the base(dictionary) form of the word. Although it is more difficult to implement, it is also stronger than just stemming. `nltk` also contains built-in lemmatizers. One of the most popular ones is called the __WordNet__ lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyPOE0zWxi-5",
        "outputId": "3c9c06f4-7115-45e2-af6d-385cb9ae2da3"
      },
      "source": [
        "#### Lemmatizing Words ####\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "my_tokens = ['eat', 'ate', 'walk', 'walked', 'buy', 'bought', 'apple', 'apples', 'knife', 'knives']\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lem_tokens = [lemmatizer.lemmatize(t) for t in my_tokens]\n",
        "\n",
        "# Print results\n",
        "print('Before lemmatizing:\\n', my_tokens)\n",
        "print('After lemmatizing:\\n', lem_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Before lemmatizing:\n",
            " ['eat', 'ate', 'walk', 'walked', 'buy', 'bought', 'apple', 'apples', 'knife', 'knives']\n",
            "After lemmatizing:\n",
            " ['eat', 'ate', 'walk', 'walked', 'buy', 'bought', 'apple', 'apple', 'knife', 'knife']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qF5nOMazBRU"
      },
      "source": [
        "We can see it still struggles with some aspects of the language (like plurals). But overall, it helps. Let's lemmatize our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiSHBlepzQev",
        "outputId": "d3d5e76b-4196-44cb-c15e-9c625e18612b"
      },
      "source": [
        "#### Lemmatize the Dataset ####\n",
        "\n",
        "lem_tokens = []\n",
        "for row in data['tokens_no_sw']:\n",
        "  lem_tokens.append([lemmatizer.lemmatize(t) for t in row])\n",
        "\n",
        "data['lem_tokens'] = lem_tokens\n",
        "\n",
        "# Print results\n",
        "print(data['lem_tokens'].head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [go, jurong, point, , crazy, available, only, ...\n",
            "1                   [ok, lar, , joking, wif, u, oni, ]\n",
            "2    [free, entry, in, 2, wkly, comp, to, win, fa, ...\n",
            "3    [u, dun, say, so, early, hor, , u, c, already,...\n",
            "4    [nah, nt, think, go, to, usf, , life, around, ...\n",
            "Name: lem_tokens, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRGjw9upzxWw"
      },
      "source": [
        "This is much better than what we had in the beginning. Note that we could also lemmatize, then stem, which would work better than either method individually.Before moving on to vectorization, I'll just do a bit more tidying. Notice that we have some tokens represented only by empty strings. I'll remove those. I'll also drop some of our intermediate columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmAdcSFD2r6z",
        "outputId": "a9f7b2e9-1b9e-4728-c319-229531176265"
      },
      "source": [
        "# Drop some intermediate columns\n",
        "list(data)\n",
        "data = data.drop(['tokens','lowercase_tokens', 'tokens_no_punct','tokens_no_sw','stemmed_tokens'], axis = 1)\n",
        "\n",
        "print('After removal:\\n', list(data))\n",
        "\n",
        "# Remove blank tokens\n",
        "no_blanks = []\n",
        "for row in data['lem_tokens']:\n",
        "  no_blanks.append([t for t in row if t != ''])\n",
        "data['tokens'] = no_blanks\n",
        "\n",
        "print(data['tokens'].head(5))\n",
        "\n",
        "# drop the last intermediate column\n",
        "data = data.drop(['lem_tokens'], axis = 1)\n",
        "\n",
        "# Now we should have only the label, original text, and cleaned token lists\n",
        "print('Current Columns:\\n',list(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removal:\n",
            " ['label', 'message', 'lem_tokens']\n",
            "0    [go, jurong, point, crazy, available, only, in...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, in, 2, wkly, comp, to, win, fa, ...\n",
            "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
            "4    [nah, nt, think, go, to, usf, life, around, he...\n",
            "Name: tokens, dtype: object\n",
            "Current Columns:\n",
            " ['label', 'message', 'tokens']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFUdMO0x6RkM"
      },
      "source": [
        "Ok great. Now we have a much cleaner dataset, which we can use to run a model. But before we do this, we need to think about how to represent text as features....\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GePUkE3oE8y"
      },
      "source": [
        "## 6. Vectorizing Token Collections\n",
        "\n",
        "<br>\n",
        "\n",
        "One of the key assumptions in NLP is called the __Distributional Hypothesis__. It says (roughly) that words used in the same context tend to have similar meanings. Put differently, 'A word is characterized by the company it keeps' (JR Firth, 1950). \n",
        "- What this means for us is that similar documents will contain similar words/tokens.\n",
        "- But how to measure this similarity?\n",
        "\n",
        "<br>\n",
        "\n",
        "__Text Vectorization__ is the process of converting text features into a numerical representation.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "__Q: What are some ways we can represent text as numbers?__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi6gOxouHh_c"
      },
      "source": [
        "<br>\n",
        "\n",
        "There are many common ways of turning our tokens into a matrix. The most popular is through the use of a __term-document matrix__, or __token-document matrix__\n",
        "- Suppose I have two documents(movie reviews):\n",
        "\n",
        "    Doc1: _'No surprises and very few laughs'_\n",
        "\n",
        "    Doc2: _'Very powerful. I was surprised.'_\n",
        "\n",
        "After preprocesssing like we did above, the term document matrix might look like the following:\n",
        "$$\n",
        "\\begin{array}{ c c c }\n",
        " Token & Doc 1 & Doc 2 \\\\ \n",
        " \\hline\n",
        " no & 1 & 0 \\\\  \n",
        " suprise & 1 & 1 \\\\\n",
        " very & 1 & 1 \\\\\n",
        " few & 1 & 0 \\\\\n",
        " laugh & 1 & 0 \\\\\n",
        " power & 0 & 1   \n",
        "\\end{array}$$\n",
        "This matrix needn't contain the presence/absence of a word. We can also populate it with word counts, or with custom measures, such as __Term-Frequency Inverse Document-Frequency (TF-IDF)__. This is equal to the product of term frequency (i.e. the number of times token t appears in document d) multiplied by the inverse of document frequency (the proportion of all documents containing token t). This measure controls for the fact that one word used many times in a small number of documents could mistakenly upweight the probabilities associated with certain classes under the BOW model.\n",
        "\n",
        "We don't have to vectorize by hand though. `Scikit-learn` has built in methods to do this.\n",
        "- Note that the following methods are also designed to do some preprocessing and tokenizing, so the default input is raw strings. Since we've already done these steps, I will use a dummy function to override these components. The output will be our term-document matrix as a Scipy sparse matrix, which we can convert to a numpy array.\n",
        "- I will then split the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsUKbiXis-92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cca901-6d64-4e25-e8f4-a0897ba7a741"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer # used in the next cell\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate labels from features, converting to numpy arrays\n",
        "X, y = data['tokens'].to_numpy(), data['label'].to_numpy()\n",
        "\n",
        "\n",
        "def override_fcn(doc):\n",
        "  # We expect a list of tokens as input\n",
        "  return doc\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vec = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Remember this output is a Scipy Sparse Array\n",
        "counts = count_vec.fit_transform(X)\n",
        "print(counts.toarray())\n",
        "\n",
        "# Print the names of each of the features (1000 total))\n",
        "print(count_vec.get_feature_names())\n",
        "# Print this mapping as dictionary\n",
        "print(count_vec.vocabulary_)\n",
        "\n",
        "## Which row represents 'great'\n",
        "print('\\nGreat is located at row: ',count_vec.vocabulary_['great'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['0800', '08000839402', '08000930705', '1', '10', '100', '1000', '10p', '10pmin', '12', '12hrs', '150', '150p', '150pmsg', '150ppm', '16', '18', '1st', '2', '2003', '2nd', '3', '4', '5', '500', '6', '7', '750', '8', '800', '8007', '86688', '87066', '9', 'abiola', 'able', 'about', 'abt', 'account', 'across', 'actually', 'address', 'admirer', 'aft', 'after', 'afternoon', 'again', 'against', 'age', 'ago', 'ah', 'aight', 'all', 'almost', 'alone', 'already', 'alright', 'also', 'always', 'amp', 'an', 'angry', 'another', 'answer', 'any', 'anyone', 'anything', 'anytime', 'anyway', 'apply', 'ard', 'area', 'around', 'as', 'asap', 'ask', 'askd', 'asked', 'asking', 'attempt', 'auction', 'available', 'await', 'award', 'awarded', 'away', 'awesome', 'b', 'b4', 'babe', 'baby', 'back', 'bad', 'bank', 'bath', 'bathe', 'bb', 'bcoz', 'bday', 'beautiful', 'bed', 'before', 'believe', 'best', 'better', 'between', 'bid', 'big', 'bill', 'birthday', 'bit', 'blue', 'bluetooth', 'bonus', 'book', 'booked', 'bored', 'bos', 'both', 'bout', 'box', 'boy', 'boytoy', 'break', 'bring', 'brother', 'bslvyl', 'bt', 'bus', 'busy', 'buy', 'c', 'ca', 'call', 'called', 'caller', 'callertune', 'calling', 'camcorder', 'came', 'camera', 'can', 'cant', 'car', 'card', 'care', 'carlos', 'case', 'cash', 'catch', 'cause', 'cd', 'chance', 'change', 'charge', 'charged', 'chat', 'cheap', 'check', 'checking', 'cheer', 'chennai', 'chikku', 'child', 'choose', 'christmas', 'claim', 'class', 'clean', 'close', 'club', 'co', 'code', 'collect', 'collection', 'college', 'colour', 'come', 'comin', 'coming', 'comp', 'company', 'complimentary', 'computer', 'confirm', 'congrats', 'congratulation', 'contact', 'content', 'cool', 'copy', 'correct', 'cost', 'could', 'couple', 'course', 'cover', 'coz', 'crave', 'crazy', 'credit', 'cum', 'currently', 'customer', 'cut', 'd', 'da', 'dad', 'darlin', 'darren', 'dat', 'date', 'dating', 'day', 'de', 'deal', 'dear', 'decided', 'decimal', 'deep', 'del', 'delivery', 'den', 'detail', 'didnt', 'die', 'different', 'difficult', 'dinner', 'direct', 'dis', 'discount', 'dnt', 'doctor', 'doesnt', 'dogging', 'doin', 'don', 'done', 'dont', 'door', 'double', 'down', 'download', 'draw', 'dream', 'drink', 'drive', 'driving', 'drop', 'drug', 'dude', 'dun', 'dunno', 'e', 'each', 'earlier', 'early', 'easy', 'eat', 'eg', 'eh', 'either', 'else', 'em', 'email', 'empty', 'end', 'ending', 'energy', 'england', 'enjoy', 'enough', 'enter', 'entered', 'entry', 'especially', 'etc', 'eve', 'even', 'evening', 'ever', 'every', 'everyone', 'everything', 'ex', 'exam', 'expires', 'extra', 'eye', 'face', 'fact', 'family', 'fancy', 'fantastic', 'fantasy', 'far', 'fast', 'father', 'feel', 'feeling', 'felt', 'few', 'figure', 'film', 'final', 'finally', 'find', 'fine', 'finish', 'finished', 'first', 'flag', 'flight', 'fone', 'food', 'forget', 'forgot', 'found', 'fr', 'free', 'freemsg', 'fri', 'friday', 'friend', 'friendship', 'frm', 'frnd', 'frnds', 'from', 'fuck', 'fucking', 'full', 'fun', 'g', 'gal', 'game', 'gap', 'gas', 'gave', 'gay', 'gd', 'get', 'gettin', 'getting', 'gift', 'girl', 'give', 'glad', 'go', 'god', 'goin', 'going', 'gon', 'gone', 'good', 'goodmorning', 'got', 'gr8', 'great', 'grin', 'gt', 'guaranteed', 'gud', 'guess', 'guy', 'gym', 'ha', 'haf', 'haha', 'hair', 'half', 'hand', 'happen', 'happened', 'happiness', 'happy', 'hard', 'hav', 'havent', 'head', 'hear', 'heard', 'heart', 'hee', 'hello', 'help', 'here', 'hey', 'hi', 'hit', 'hmm', 'hmmm', 'hmv', 'ho', 'hold', 'holiday', 'home', 'hope', 'hoping', 'hospital', 'hot', 'hotel', 'hour', 'house', 'how', 'hows', 'hr', 'http', 'huh', 'hungry', 'hurt', 'id', 'idea', 'identifier', 'ill', 'im', 'immediately', 'important', 'in', 'india', 'info', 'information', 'into', 'invited', 'ipod', 'jay', 'job', 'john', 'join', 'joke', 'joy', 'jus', 'just', 'juz', 'k', 'kate', 'keep', 'kick', 'kid', 'kind', 'kiss', 'knew', 'know', 'knw', 'lady', 'land', 'landline', 'laptop', 'lar', 'last', 'late', 'later', 'latest', 'laugh', 'lazy', 'ldn', 'le', 'leaf', 'least', 'leave', 'leaving', 'lect', 'left', 'leh', 'lei', 'lesson', 'let', 'liao', 'life', 'light', 'like', 'line', 'link', 'listen', 'little', 'live', 'll', 'load', 'loan', 'log', 'lol', 'long', 'look', 'looking', 'lor', 'lose', 'lost', 'lot', 'lovable', 'love', 'loved', 'lovely', 'lover', 'loving', 'lt', 'ltd', 'luck', 'lucky', 'lunch', 'luv', 'm', 'made', 'mah', 'mail', 'make', 'making', 'man', 'many', 'march', 'match', 'mate', 'may', 'mayb', 'maybe', 'mean', 'meant', 'med', 'meet', 'meeting', 'meh', 'member', 'men', 'merry', 'message', 'met', 'might', 'min', 'mind', 'mine', 'minute', 'miss', 'missed', 'missing', 'mistake', 'mm', 'mob', 'mobile', 'mobileupd8', 'mode', 'mom', 'moment', 'monday', 'money', 'month', 'more', 'morning', 'most', 'motorola', 'move', 'movie', 'mr', 'mrng', 'mrt', 'msg', 'mu', 'much', 'mum', 'music', 'must', 'muz', 'n', 'na', 'nah', 'name', 'national', 'near', 'need', 'network', 'neva', 'never', 'new', 'news', 'next', 'ni8', 'nice', 'night', 'nite', 'no', 'no1', 'nobody', 'noe', 'nokia', 'none', 'noon', 'nope', 'normal', 'not', 'nothing', 'now', 'nt', 'ntt', 'number', 'nyt', 'o', 'off', 'offer', 'office', 'oh', 'ok', 'okay', 'okie', 'old', 'on', 'once', 'one', 'online', 'only', 'oops', 'open', 'operator', 'opinion', 'opt', 'optout', 'orange', 'orchard', 'order', 'oredi', 'oso', 'other', 'others', 'otherwise', 'out', 'outside', 'over', 'own', 'p', 'pa', 'page', 'pain', 'paper', 'parent', 'park', 'part', 'party', 'pas', 'password', 'pay', 'people', 'per', 'person', 'pete', 'phone', 'photo', 'pic', 'pick', 'picking', 'place', 'plan', 'play', 'player', 'please', 'pls', 'plus', 'plz', 'pm', 'po', 'pobox', 'point', 'police', 'poly', 'poor', 'possible', 'post', 'pound', 'press', 'pretty', 'price', 'princess', 'private', 'prize', 'prob', 'probably', 'problem', 'project', 'promise', 'pub', 'put', 'question', 'quite', 'quiz', 'r', 'rate', 're', 'reach', 'reached', 'read', 'reading', 'ready', 'real', 'really', 'reason', 'receive', 'red', 'remember', 'remove', 'rent', 'rental', 'reply', 'representative', 'request', 'rest', 'return', 'reward', 'right', 'ring', 'ringtone', 'rite', 'road', 'rock', 'room', 'rply', 'run', 's', 'sad', 'sae', 'safe', 'said', 'same', 'sat', 'saturday', 'savamob', 'save', 'saw', 'say', 'saying', 'sch', 'school', 'sea', 'search', 'second', 'secret', 'see', 'seeing', 'seems', 'seen', 'selected', 'sell', 'semester', 'send', 'sending', 'sent', 'seriously', 'service', 'set', 'sex', 'sexy', 'shall', 'shes', 'shit', 'shop', 'shopping', 'short', 'should', 'show', 'shower', 'si', 'side', 'sim', 'simple', 'since', 'single', 'sir', 'sister', 'situation', 'sleep', 'sleeping', 'slow', 'slowly', 'sm', 'small', 'smile', 'smiling', 'smoke', 'smth', 'snow', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'song', 'soon', 'sorry', 'sort', 'sound', 'speak', 'special', 'spend', 'st', 'start', 'started', 'statement', 'stay', 'std', 'still', 'stop', 'store', 'story', 'study', 'stuff', 'stupid', 'sub', 'summer', 'sun', 'support', 'supposed', 'sure', 'surprise', 'sweet', 'swing', 't', 'ta', 'take', 'taking', 'talk', 'talking', 'tc', 'teach', 'tel', 'tell', 'telling', 'ten', 'term', 'test', 'text', 'th', 'than', 'thank', 'thanks', 'thanx', 'thats', 'then', 'there', 'thing', 'think', 'thinking', 'thk', 'tho', 'though', 'thought', 'through', 'ticket', 'til', 'till', 'time', 'tired', 'tmr', 'to', 'today', 'together', 'told', 'tomo', 'tomorrow', 'tone', 'tonight', 'too', 'took', 'top', 'tot', 'touch', 'town', 'train', 'treat', 'tried', 'trip', 'true', 'truth', 'try', 'trying', 'turn', 'tv', 'two', 'txt', 'txting', 'txts', 'type', 'u', 'ugh', 'uk', 'uncle', 'understand', 'unless', 'unlimited', 'unredeemed', 'unsubscribe', 'up', 'update', 'ur', 'urgent', 'urself', 'use', 'used', 'user', 'usf', 'using', 'v', 'valentine', 'valid', 'valued', 've', 'very', 'via', 'video', 'visit', 'voice', 'voucher', 'w', 'wait', 'waiting', 'wake', 'walk', 'wan', 'wana', 'want', 'wanted', 'wap', 'warm', 'wat', 'watch', 'watching', 'water', 'way', 'week', 'weekend', 'weekly', 'welcome', 'well', 'wen', 'went', 'whatever', 'whats', 'when', 'whenever', 'where', 'whole', 'why', 'wid', 'wif', 'wife', 'wil', 'will', 'win', 'wine', 'winner', 'wish', 'wishing', 'wit', 'with', 'within', 'without', 'wk', 'wkly', 'wo', 'woke', 'won', 'wonder', 'wonderful', 'wont', 'word', 'work', 'working', 'world', 'worried', 'worry', 'worth', 'wot', 'would', 'wow', 'write', 'wrong', 'x', 'xmas', 'xx', 'xxx', 'y', 'ya', 'yar', 'yeah', 'year', 'yep', 'yes', 'yesterday', 'yet', 'yo', 'yr', 'yup', 'å100', 'å1000', 'å150', 'å2000', 'å250', 'å350', 'å500', 'å5000', 'ì_', 'ìï', 'û_', 'ûò']\n",
            "{'go': 349, 'point': 658, 'crazy': 199, 'available': 81, 'only': 607, 'in': 419, 'n': 561, 'great': 359, 'world': 963, 'e': 254, 'there': 834, 'got': 357, 'wat': 919, 'ok': 599, 'lar': 449, 'wif': 939, 'u': 878, 'free': 320, 'entry': 275, '2': 18, 'wkly': 953, 'comp': 180, 'to': 849, 'win': 943, 'final': 305, 'may': 512, 'text': 826, 'receive': 692, 'question': 678, 'std': 795, 'txt': 874, 'rate': 682, 't': 812, 'c': 131, 's': 713, 'apply': 69, 'dun': 252, 'say': 724, 'so': 776, 'early': 257, 'already': 55, 'then': 833, 'nah': 563, 'nt': 590, 'think': 836, 'usf': 895, 'life': 469, 'around': 72, 'here': 387, 'though': 840, 'freemsg': 321, 'hey': 388, '3': 21, 'week': 924, 'now': 589, 'no': 578, 'word': 960, 'back': 91, 'd': 205, 'like': 471, 'some': 777, 'fun': 333, 'up': 887, 'still': 796, 'xxx': 975, 'send': 739, 'å150': 990, 'even': 279, 'brother': 125, 'not': 587, 'speak': 787, 'with': 949, 'treat': 864, 'per': 639, 'request': 700, 'set': 744, 'callertune': 136, 'all': 52, 'caller': 135, 'press': 665, '9': 33, 'copy': 190, 'friend': 324, 'winner': 945, 'valued': 900, 'network': 568, 'customer': 203, 'selected': 736, 'prize': 670, 'reward': 703, 'claim': 166, 'call': 133, 'code': 172, 'valid': 899, '12': 9, 'hour': 403, 'mobile': 537, 'month': 544, 'more': 545, 'r': 681, 'update': 888, 'latest': 453, 'colour': 176, 'camera': 140, 'co': 171, 'on': 603, 'm': 501, 'gon': 353, 'na': 562, 'home': 397, 'soon': 783, 'want': 915, 'talk': 816, 'about': 36, 'stuff': 801, 'tonight': 856, 'k': 435, 've': 901, 'enough': 272, 'today': 850, 'chance': 152, 'cash': 148, 'from': 329, '100': 5, 'pound': 664, 'cost': 192, '16': 15, 'reply': 698, '4': 22, 'info': 421, 'urgent': 890, 'won': 956, '1': 3, 'pobox': 657, 'right': 704, 'thank': 829, 'promise': 675, 'wont': 959, 'take': 814, 'help': 386, 'will': 942, 'wonderful': 958, 'time': 846, 'date': 211, 'use': 892, 'credit': 200, 'wap': 917, 'link': 473, 'next': 573, 'message': 524, 'http': 408, 'oh': 598, 'watching': 921, 'eh': 261, 'remember': 694, 'how': 405, 'name': 564, 'yes': 982, 'v': 897, 'make': 505, 'fine': 308, 'way': 923, 'feel': 299, 'b': 87, 'england': 270, 'dont': 239, 'miss': 531, 'news': 572, 'ur': 889, 'national': 565, 'eg': 260, 'try': 869, 'seriously': 742, 'going': 352, 'ha': 367, 'ì_': 996, 'pay': 637, 'first': 311, 'when': 933, 'da': 206, 'comin': 178, 'aft': 43, 'finish': 309, 'lunch': 499, 'down': 242, 'lor': 485, 'ard': 70, 'smth': 774, 'alright': 56, 'can': 141, 'meet': 518, 'just': 433, 'eat': 259, 'really': 690, 'hungry': 410, 'tho': 839, 'getting': 344, 'worried': 964, 'know': 443, 'turn': 871, 'lol': 481, 'always': 58, 'catch': 149, 'bus': 128, 'mom': 540, 'left': 463, 'over': 624, 'dinner': 228, 'love': 490, 'amp': 59, 're': 683, 'car': 143, 'll': 477, 'let': 467, 'room': 710, 'work': 961, 'wait': 909, 'sure': 808, 'why': 937, 'x': 972, 'live': 476, 'yeah': 979, 'out': 622, 'child': 163, 'till': 845, 'wo': 954, 'too': 857, 'cheer': 160, 'tell': 821, 'anything': 66, 'thanks': 830, 'ringtone': 706, 'uk': 880, 'charged': 155, 'please': 651, 'confirm': 184, 'yup': 987, 'look': 483, 'msg': 554, 'again': 46, '2nd': 20, 'lesson': 466, 'oops': 608, 'done': 238, 'see': 732, 'hello': 385, 'saturday': 720, 'decided': 217, 'tomo': 853, 'trying': 870, 'pls': 652, 'wanted': 916, 'weekend': 925, 'abiola': 34, 'forget': 316, 'need': 567, 'crave': 198, 'most': 547, 'sweet': 810, 'tried': 865, 'sm': 769, 'nokia': 582, 'camcorder': 138, '08000930705': 2, 'delivery': 221, 'tomorrow': 854, 'seeing': 733, 'hope': 398, 'man': 507, 'well': 928, 'lt': 495, 'gt': 361, 'get': 342, 'ca': 132, 'could': 193, 'maybe': 514, 'ask': 75, 'bit': 110, 'hospital': 400, 'telling': 822, 'saw': 723, 'class': 167, 'run': 712, 'half': 371, 'almost': 53, 'whole': 936, 'second': 730, 'morning': 546, 'place': 647, 'wow': 969, 'never': 570, 'thought': 841, 'since': 760, 'best': 103, 'happy': 376, 'sorry': 784, 'give': 347, 'new': 571, 'red': 693, 'play': 649, 'correct': 191, 'end': 267, 'yesterday': 983, 'find': 307, 'congrats': 185, 'year': 980, 'special': 788, 'pas': 635, 'etc': 277, 'later': 452, 'meeting': 519, 'where': 935, 'reached': 685, 'pick': 645, 'move': 549, 'pain': 629, 'good': 355, 'joke': 430, 'girl': 346, 'situation': 764, 'part': 633, 'checking': 159, 'took': 858, 'come': 177, 'double': 241, 'check': 158, 'hair': 370, 'said': 717, 'cut': 204, 'short': 752, 'nice': 575, 'mob': 536, 'awarded': 84, 'bonus': 113, 'song': 782, 'day': 213, 'frnds': 328, 'rply': 711, 'complimentary': 182, 'trip': 866, 'å1000': 989, 'dis': 230, '10': 4, 'hear': 381, 'lucky': 498, 'save': 722, 'money': 543, 'hee': 384, 'finished': 310, 'hi': 389, 'babe': 89, 'im': 416, 'wan': 913, 'something': 780, 'xx': 974, 'waiting': 910, 'once': 604, 'thats': 832, 'cool': 189, 'people': 638, 'very': 902, 'much': 556, 'pa': 627, 'after': 44, 'same': 718, 'looking': 484, 'job': 427, 'ta': 813, 'ah': 50, 'stop': 797, 'real': 689, 'yo': 985, 'ticket': 843, 'one': 605, 'used': 893, 'started': 792, 'came': 139, 'bed': 100, 'download': 243, 'wen': 929, 'close': 169, 'another': 62, 'night': 576, 'late': 451, 'afternoon': 45, 'mean': 515, 'any': 64, 'y': 976, 'smile': 771, 'hurt': 411, 'someone': 779, 'smiling': 772, 'service': 743, 'representative': 699, '0800': 0, 'between': 105, 'guaranteed': 362, 'å5000': 995, 'havent': 379, 'buy': 130, 'show': 754, 'password': 636, 'abt': 37, 'load': 478, 'loan': 479, 'wk': 952, 'forgot': 317, 'shower': 755, 'cause': 150, 'prob': 671, 'nothing': 588, 'else': 263, 'okay': 600, 'price': 667, 'long': 482, 'gone': 354, 'driving': 248, 'test': 825, 'yet': 984, 'guess': 364, 'gave': 339, 'men': 522, 'search': 729, 'page': 628, 'lot': 488, 'dear': 216, 'wish': 946, 'birthday': 109, 'making': 506, 'aight': 51, 'hit': 390, 'would': 968, 'address': 41, 'computer': 183, 'old': 602, 'better': 104, 'worry': 965, 'busy': 129, 'thing': 835, 'mah': 503, 'contact': 187, 'last': 450, 'draw': 244, '12hrs': 10, '150ppm': 14, 'anyway': 68, 'juz': 434, 'haha': 369, 'happened': 374, 'entered': 274, 'bday': 98, 'bos': 117, 'felt': 301, 'askd': 76, 'invited': 424, 'went': 930, 'holiday': 396, 'flight': 313, 'operator': 610, '18': 16, 'must': 559, 'friday': 323, 'hmm': 391, 'uncle': 881, 'school': 727, 'food': 315, 'private': 669, 'account': 38, 'statement': 793, 'unredeemed': 885, 'identifier': 414, 'expires': 287, 'å2000': 991, 'landline': 447, 'number': 592, 'ending': 268, 'award': 83, 'match': 510, 'mu': 555, 'ìï': 997, 'sent': 741, 'sending': 740, 'should': 753, 'del': 220, 'answer': 63, 'quiz': 680, 'top': 859, 'player': 650, 'dogging': 235, 'direct': 229, 'join': 429, 'bt': 127, 'txting': 875, '150p': 12, 'haf': 368, 'chat': 156, 'age': 48, 'yr': 986, 'lazy': 455, 'type': 877, 'lect': 462, 'sir': 762, 'mail': 504, 'tired': 847, 'little': 475, 'lovable': 489, 'person': 640, 'heart': 383, 'gud': 363, 'ni8': 574, 'open': 609, 'ya': 977, 'whats': 932, 'taking': 815, 'sexy': 746, 'luv': 500, 'ltd': 496, 'hard': 377, 'wine': 944, 'thk': 838, 'dream': 245, 'without': 951, 'joy': 431, 'tv': 872, 'ill': 415, 'leaving': 461, 'house': 404, 'boy': 121, 'missing': 533, 'keep': 437, 'safe': 716, 'everyone': 283, 'parent': 631, 'hand': 372, 'each': 255, 'spend': 789, 'frnd': 327, 'order': 616, 'content': 188, 'wit': 948, 'fancy': 293, 'also': 57, 'bank': 93, 'hmmm': 392, 'muz': 560, 'liao': 468, 'coming': 179, 'cant': 142, 'believe': 102, 'mr': 551, 'bath': 94, 'carlos': 146, 'til': 844, 'smoke': 773, 'worth': 966, 'doesnt': 234, 'log': 480, 'offer': 596, 'especially': 276, 'gr8': 358, 'guy': 365, 'working': 962, 'boytoy': 122, 'awesome': 86, 'minute': 530, 'xmas': 973, 'jus': 432, 'bathe': 95, 'si': 756, 'using': 896, 'touch': 861, 'deal': 215, 'finally': 306, 'course': 195, 'stay': 794, 'able': 35, 'every': 282, 'mrng': 552, 'hav': 378, 'story': 799, 'tmr': 848, 'orchard': 615, 'mrt': 553, 'kate': 436, 'evening': 280, 'found': 318, 'darlin': 208, 'college': 175, 'decimal': 218, 'id': 412, 'goodmorning': 356, 'sleeping': 766, 'dat': 210, 'oso': 618, 'oredi': 617, 'before': 101, 'bill': 108, 'both': 118, 'big': 107, 'ready': 688, 'break': 123, 'semester': 738, 'study': 800, 'noe': 581, 'leh': 464, 'sound': 786, 'few': 302, 'easy': 258, 'exam': 286, 'march': 509, 'o': 594, 'called': 134, 'important': 418, 'shop': 750, 'happen': 373, 'nite': 577, '500': 24, 'collect': 173, 'off': 595, 'start': 791, 'g': 334, 'company': 181, 'po': 656, 'than': 828, 'bcoz': 97, 'teach': 819, 'walk': 912, 'road': 708, 'side': 757, '10p': 7, 'wil': 941, 'reach': 684, 'kick': 438, 'secret': 731, 'admirer': 42, 'laptop': 448, 'case': 147, 'tel': 820, 'meant': 516, 'told': 852, 'into': 423, 'face': 290, 'watch': 920, 'fr': 319, 'thanx': 831, 'everything': 284, 'asked': 77, 'didnt': 224, 'wake': 911, 'missed': 532, 'sleep': 765, 'congratulation': 186, 'cd': 151, 'voucher': 907, 'music': 558, '87066': 32, 'hold': 395, 'angry': 61, 'wid': 938, 'dnt': 232, 'coz': 197, 'true': 867, 'deep': 219, 'care': 145, 'other': 619, 'lover': 493, 'video': 904, '750': 27, 'anytime': 67, 'min': 527, 'unlimited': 884, 'shopping': 751, 'own': 625, 'ring': 705, 'hot': 401, 'unsubscribe': 886, 'wana': 914, 'plan': 648, 'choose': 164, 'club': 170, 'single': 761, 'charge': 154, 'leaf': 458, 'blue': 111, 'hmv': 393, '86688': 31, 'might': 526, 'full': 332, 'swing': 811, 'far': 296, 'okie': 601, 'baby': 90, 'fone': 314, 'shall': 747, 'stupid': 802, 'phone': 642, 'sim': 758, 'card': 144, 'unless': 883, 'die': 225, 'plz': 654, 'bslvyl': 126, 'somebody': 778, 'shit': 749, 'somewhere': 781, 'book': 114, 'friendship': 325, 'game': 336, 'tone': 855, 'sister': 763, 'weekly': 926, 'box': 120, 'normal': 586, 'rest': 701, 'wot': 967, 'lost': 487, 'made': 502, 'dunno': 253, 'dude': 251, 'merry': 523, 'christmas': 165, 'kiss': 441, 'pete': 641, 'problem': 673, 'reading': 687, 'read': 686, 'light': 470, 'movie': 550, 'return': 702, 'immediately': 417, 'line': 472, 'via': 903, 'valentine': 898, '150pmsg': 13, 'calling': 137, 'post': 663, '1000': 6, 'two': 873, 'tc': 818, 'small': 770, 'txts': 876, '150': 11, 'ever': 281, 'urself': 891, 'figure': 303, 'jay': 426, '5': 23, 'ago': 49, 'met': 525, 'ex': 285, 'currently': 202, 'moment': 541, 'st': 790, 'chikku': 162, 'motorola': 548, 'bluetooth': 112, 'orange': 614, 'mobileupd8': 538, '08000839402': 1, 'discount': 231, 'woke': 955, 'talking': 817, 'seen': 735, 'project': 674, 'mistake': 534, 'quite': 679, 'slow': 767, 'reason': 691, 'couple': 194, 'leave': 460, 'mm': 535, 'rental': 697, 'huh': 409, 'sat': 719, 'office': 597, 'bout': 119, 'actually': 40, 'rock': 709, 'put': 677, 'as': 73, 'god': 350, 'india': 420, 'change': 153, 'poly': 660, '1st': 17, 'none': 583, 'yep': 981, 'pm': 655, 'drink': 246, 'den': 222, 'bring': 124, 'dating': 212, 'å250': 992, 'head': 380, 'eve': 278, 'land': 446, 'voice': 906, '7': 26, 'mind': 528, 'fucking': 331, 'ldn': 456, 'booked': 115, 'th': 827, 'ten': 823, 'gd': 341, 'supposed': 807, 'tot': 860, 'detail': 223, 'å500': 994, 'welcome': 927, 'beautiful': 99, 'kind': 440, 'asking': 78, 'bad': 92, 'different': 226, 'optout': 613, 'princess': 668, 'enjoy': 271, 'many': 508, 'sae': 715, 'remove': 695, 'cum': 201, 'thinking': 837, 'term': 824, 'visit': 905, '6': 25, 'meh': 520, 'nope': 585, 'monday': 542, 'either': 262, 'lose': 486, 'water': 922, 'bored': 116, 'outside': 623, 'park': 632, 'near': 566, 'rent': 696, 'opinion': 611, '8': 28, 'simple': 759, 'ipod': 425, 'le': 457, 'fri': 322, 'attempt': 79, 'savamob': 721, 'member': 521, 'sub': 803, 'pretty': 666, 'lady': 445, 'within': 950, '2003': 19, '800': 29, 'hoping': 399, 'across': 39, 'sea': 728, 'probably': 672, 'listen': 474, 'mine': 529, 'hotel': 402, 'through': 842, 'warm': 918, 'cheap': 157, 'online': 606, 'pic': 644, 'fast': 297, 'fuck': 330, 'gym': 366, 'whatever': 931, 'yar': 978, 'mum': 557, 'sch': 726, 'clean': 168, 'door': 240, 'kid': 439, 'em': 264, 'train': 863, 'noon': 584, 'neva': 569, 'family': 292, 'happiness': 375, 'snow': 775, 'together': 851, 'pub': 676, 'darren': 209, 'area': 71, 'å350': 993, 'ûò': 999, 'dad': 207, 'email': 265, 'town': 862, 'drug': 250, 'sun': 805, 'paper': 630, 'de': 214, 'ho': 394, 'laugh': 454, 'opt': 612, 'saying': 725, 'knw': 444, 'sex': 745, 'an': 60, 'gap': 337, 'û_': 998, 'idea': 413, 'away': 85, 'fantastic': 294, 'w': 908, 'seems': 734, 'sell': 737, 'b4': 88, 'feeling': 300, 'mayb': 513, 'rite': 707, 'gal': 335, 'glad': 348, 'eye': 289, 'anyone': 65, 'possible': 662, 'wishing': 947, 'summer': 804, 'store': 798, 'goin': 351, 'wonder': 957, 'drive': 247, 'picking': 646, 'lovely': 492, 'no1': 579, 'mate': 511, 'ugh': 879, 'gift': 345, 'å100': 988, 'bid': 106, 'gettin': 343, 'extra': 288, '8007': 30, 'understand': 882, 'await': 82, 'collection': 174, 'poor': 661, 'whenever': 934, 'sort': 785, 'user': 894, 'asap': 74, 'drop': 249, 'otherwise': 621, 'ntt': 591, 'knew': 442, 'sad': 714, 'plus': 653, 'gay': 340, '10pmin': 8, 'wrong': 971, 'photo': 643, 'least': 459, 'truth': 868, 'against': 47, 'wife': 940, 'earlier': 256, 'loving': 494, 'party': 634, 'heard': 382, 'frm': 326, 'don': 237, 'nyt': 593, 'support': 806, 'shes': 748, 'p': 626, 'doctor': 233, 'information': 422, 'surprise': 809, 'grin': 360, 'film': 304, 'luck': 497, 'energy': 269, 'chennai': 161, 'enter': 273, 'nobody': 580, 'mode': 539, 'others': 620, 'bb': 96, 'fantasy': 295, 'write': 970, 'cover': 196, 'fact': 291, 'alone': 54, 'loved': 491, 'hows': 406, 'auction': 80, 'difficult': 227, 'gas': 338, 'john': 428, 'lei': 465, 'father': 298, 'doin': 236, 'slowly': 768, 'flag': 312, 'med': 517, 'empty': 266, 'police': 659, 'hr': 407}\n",
            "\n",
            "Great is located at row:  359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y16poRsBU3yT"
      },
      "source": [
        "- Note that TFIDF __transformer__ requires you to first run `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npCbiPXxSvNU",
        "outputId": "c4f51689-326f-4c30-f4e0-f19a396aff98"
      },
      "source": [
        "#### TF-IDF Vectorize ####\n",
        "\n",
        "# Note that smoothing is done by default\n",
        "tfidf = TfidfTransformer()\n",
        "\n",
        "tfs = tfidf.fit_transform(counts)\n",
        "\n",
        "print(tfs.toarray())\n",
        "\n",
        "# Let's use the TFIDF counts for modelling\n",
        "X = tfs.toarray()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# First three rows of training features and labels\n",
        "print('First 3 features:\\n',X_train[:3])\n",
        "print('First 3 labels:\\n',y_train[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "First 3 features:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "First 3 labels:\n",
            " [0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqASO5dQn8Wd"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 7. Binary Classification with the Naive Bayes Model\n",
        "\n",
        "__The Basic Setup__\n",
        "\n",
        "- We're interested in classifying our documents into two classes. Let's call these classes $S$ and $H$, for Spam and Ham. How might we use our tokens to predict the class?\n",
        "- __Idea__: Let's try to model the probabilities of each class __conditional on the tokens in the document__. Suppose our message has D tokens. It might not be obvious how to do this, but we can rely on Bayes Rule:\n",
        "\n",
        "$$ Pr(S|w_1 , \\cdots , w_D)  = \\frac{Pr(w_1 , \\cdots , w_D | S) Pr(S)}{Pr(w_1 , \\cdots , w_D)} \\propto Pr(w_1 , \\cdots , w_D | S) Pr(S) $$\n",
        "\n",
        "So the probability that our message is spam is equal to the probability that we get these words given the message is spam, multiplied by the probability of any message being spam (this is pretty intuitive).\n",
        "\n",
        "- __Problem__: How in the world do we estimate $Pr(w_1 , \\cdots , w_D | S)$?\n",
        "\n",
        "- __Solution__: We use a __naive__ assumption. Let's assume that words are independent of each other conditional on class (this is obviously not true in reality, but it makes our lives easier). Then we can rewrite our probability:\n",
        "\n",
        "$$ Pr(S|w_1 , \\cdots , w_D)  = \\propto \\Pi_{i=1}^{D}Pr(w_i | S) Pr(S) $$\n",
        "\n",
        "Now we have something we can estimate. Assume V is the set of all words in your training corpus. The __Naive Bayes__ approach does the following:\n",
        "- 1. Estimate $\\hat{Pr}(S) = \\frac{N_s}{N}$, where $N_s$ is the number of documents labelled as S. Similarly estimate $\\hat{Pr}(H) = 1 - \\hat{Pr}(S)$.\n",
        "- 2. For every token in every document, estimate $\\hat{Pr}(w_i|S) = \\frac{count(w_i|S)}{\\Sigma_{w \\in V} count(w_i|S)}$, where $count(w_i|S)$ is the number of times token $w_i$ appears __in all spam documents__, and $\\Sigma_{w \\in V} count(w_i|S)$ is the total number of words in all spam documents. Do a similar computation for Ham documents and tokens. \n",
        "- 3. Once we have these probabilities, we can compute $\\hat{Pr}(S|w_1 , \\cdots , w_D)$ and $\\hat{Pr}(H|w_1 , \\cdots , w_D)$ for any new document. Then we simply label that document as Spam or Ham, depending on which probability is larger.\n",
        "\n",
        "__One small problem__: If a word in the test corpus does not appear in the training corpus, it will have a count (and thus a probability) of zero. This will make the entire $\\hat{Pr}(S|w_1 , \\cdots , w_D) = 0$, even if other words in the document still have positive probability. To deal with this, we use __smoothing__, assigning every word an arbitrarily low probability. There are different ways to do this, but here is a technique called __Laplace Smoothing__:\n",
        "$$\\hat{Pr}(w_i|S) = \\frac{count(w_i|S) + 1}{\\Sigma_{w \\in V} (count(w_i|S) + 1)}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "- This probability estimation approach is called __Bag of Words (BOW)__, because we treat classes and documents as collections of words, where order does not matter.\n",
        "\n",
        "This can easily be extended to the multiclass setting (which is on your assignment). But don't worry! `Scikit-learn` handles all of these calculations for you. For the case of TFIDF data, we would simply replace $count()$ in the above formula with the TFIDF score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45NPx7JpHLIs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "4c6422ba-eac1-4b51-df42-07d0a0f41705"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let's fit the Naive Bayes model to our training data\n",
        "nb = MultinomialNB()\n",
        "# Fit model to training data\n",
        "nb.fit(X_train, y_train)\n",
        "# Predict on test data\n",
        "y_preds = nb.predict(X_test)\n",
        "\n",
        "print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test,y_preds))\n",
        "# We get a very good accuracy, despite class imbalance (recall 87% were Ham)\n",
        "\n",
        "\n",
        "# # Plot the ROC curve  - we will cover this in more detail in the next few lectures! (not included in student version)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_preds, pos_label = 1)\n",
        "\n",
        "plt.plot(fpr,tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operator Characteristic (ROC) Curve')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with simple Naive Bayes: 0.9730941704035875\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7mfqKARYz4U"
      },
      "source": [
        "That was a lot of content in a short period of time.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "# __QUESTIONS?__"
      ]
    }
  ]
}